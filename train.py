

from pytorch_lightning import Trainer
from pytorch_lightning.loggers import WandbLogger
from trainer import SpeechLLMLightning
from dataset import InstructionalAudioDataset, MyCollator, CompositeAudioDataset
from pytorch_lightning.strategies import DDPStrategy

import torch.utils.data as data_utils
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping
import wandb
import argparse
import os

import torch
from utils import get_model_config

def make_weighted_sampler_from_dataset(dataset, dtype=torch.double):
    """
    Build a WeightedRandomSampler from a CompositeAudioDataset.
    - If dataset contains only one sub-dataset: return None (use shuffle=True outside).
    - If multiple sub-datasets: expand dataset.datasets_weights into per-sample weights.
    
    Args:
        dataset: CompositeAudioDataset or similar object containing .dataset.datasets and .datasets_weights
        dtype: torch dtype for the weights tensor (default: torch.double)
    
    Returns:
        WeightedRandomSampler if multiple sub-datasets, otherwise None.
    """
    # Get the list of sub-datasets (in case of ConcatDataset)
    subs = getattr(getattr(dataset, "dataset", None), "datasets", None)

    # If not multiple sub-datasets, return None (then set shuffle=True)
    if not (isinstance(subs, (list, tuple)) and len(subs) > 1):
        return None

    sizes = [len(d) for d in subs]
    dataset_weights = getattr(dataset, "datasets_weights", None)
    if dataset_weights is None or len(dataset_weights) != len(sizes):
        raise ValueError("datasets_weights is missing or does not match the number of sub-datasets.")

    # Expand per-dataset weight to per-sample weight
    weights_per_sample = torch.cat([
        torch.full((sz,), float(w) / sz, dtype=dtype)
        for w, sz in zip(dataset_weights, sizes)
    ])
    assert len(weights_per_sample) == len(dataset), "weights length must match the total number of samples."

    return data_utils.WeightedRandomSampler(weights_per_sample,
                                  num_samples=len(weights_per_sample),
                                  replacement=True)


if __name__ == "__main__":
    model_config = get_model_config()
    
    wandb.init(project="speechllm", name=model_config['log_path'], group=model_config['group'])
    logger = WandbLogger(project="speechllm", name=model_config['log_path'], group=model_config['group'])

    print(model_config)

    model = SpeechLLMLightning(**model_config)
    tokenizer = model.llm_tokenizer

    train_dataset = CompositeAudioDataset(
        list_of_datasets=model_config['train_sets'],
        mode='train', 
        random_keys_prob=0.2,
        max_len=model_config['max_number_seconds']
        )

    val_dataset = CompositeAudioDataset(
        list_of_datasets = model_config['dev_sets'],
        mode='test',
        max_len=model_config['max_number_seconds'],
        max_size=model_config['max_size_per_dev_set']
        )

    print(f"Train set:{len(train_dataset)}, val set:{len(val_dataset)}, batch size:{model_config['batch_size']}")
    num_workers=3 #put to 0 for debugging
    my_collator = MyCollator(model_config['audio_encoder_name'], tokenizer)
    # sampler = data_utils.WeightedRandomSampler(train_dataset.datasets_weights, num_samples=len(train_dataset.datasets_weights), replacement=True)
    # Check whether it is generated by Composite / ConcatDataset
    sampler = make_weighted_sampler_from_dataset(train_dataset)
    shuffle = sampler is None  # If sampler is used, shuffle must be False
    train_loader = data_utils.DataLoader(train_dataset, batch_size=model_config['batch_size'], shuffle=shuffle, sampler=sampler, collate_fn=my_collator, num_workers=num_workers)
    val_loader = data_utils.DataLoader(val_dataset, batch_size=model_config['batch_size'], shuffle=False, collate_fn=my_collator, num_workers=num_workers)

    checkpoint_callback = ModelCheckpoint(
                    dirpath=f"checkpoints/{model_config['model_name']}", 
                    filename=model_config['model_name']+'epoch-{epoch}', 
                    save_top_k=1, 
                    monitor="val/loss", 
                    save_last=True,
                    every_n_epochs=2)
    early_stop_callback = EarlyStopping(monitor="val/loss", min_delta=0.00, patience=10, verbose=False, mode="min")

    trainer = Trainer(
            max_epochs=model_config['total_training_epoch'], 
            devices=1, accelerator="gpu", 
            strategy=DDPStrategy(find_unused_parameters=model_config['finetune_encoder']),
            limit_train_batches=model_config['train_batch_per_epoch'], 
            log_every_n_steps=100, 
            enable_checkpointing=True, 
            callbacks=[checkpoint_callback],
            fast_dev_run=False, logger=logger, 
            accumulate_grad_batches=model_config['grad_accumulate_steps']
    )
    trainer.fit(model, train_loader, val_loader)

